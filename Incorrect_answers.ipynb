{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyPRow9TcgmXaGwfM6AYn0Tp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Incorrect Answers Data Analysis"],"metadata":{"id":"U5B4_imBt2J-"}},{"cell_type":"markdown","source":["##**Overview**\n","\n","Find attached a JSON file containing data about some quiz questions of a hypothetical EdTech startup. Each question has text and a percent correct value, which is the percent of students who have answered that question correctly. The file should be encoded in UTF-8."],"metadata":{"id":"iie_jH3K-eYK"}},{"cell_type":"markdown","source":["## What words or phrases appear more frequently in questions that students tend to do poorly on?"],"metadata":{"id":"KFQZ9KI046tr"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"oLqOpyyNVYeE"},"outputs":[],"source":["# Import google drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# Import Libraries\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Import NLTK Libraries\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from nltk.stem import WordNetLemmatizer\n","import string\n","import re"],"metadata":{"id":"YXJCIrpmdKuC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Overview**\n","\n","Find attached a JSON file containing data about some quiz questions of a hypothetical EdTech startup. Each question has text and a percent correct value, which is the percent of students who have answered that question correctly. The file should be encoded in UTF-8."],"metadata":{"id":"8LtZHlqWyFwe"}},{"cell_type":"markdown","source":["## Read the Data"],"metadata":{"id":"AWAdMDW-xVHu"}},{"cell_type":"code","source":["# Convert the json data to a dataframe\n","\n","import pandas as pd\n","\n","df = pd.read_json('/content/drive/MyDrive/Text Analytics/Mini_project_1/quiz_question_data.json')\n","\n","df.to_string()\n","\n","df.head()"],"metadata":{"id":"AaXie_E-kW6H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data description"],"metadata":{"id":"MZ7IXyc_zmsR"}},{"cell_type":"code","source":["df.info()"],"metadata":{"id":"HdN_Kl5-ws-I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.isnull( ).sum()"],"metadata":{"id":"wk3ywt64w4FI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.describe()"],"metadata":{"id":"fkAcBYffwxq3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Map the values less than 0.5 as incorrect and greater than 0.5 as correct"],"metadata":{"id":"7Vs1Z-ZFzHko"}},{"cell_type":"code","source":["df['is_correct'] = df['percent_correct'].map(lambda x: \"Correct\" if x > 0.5 else \"Incorrect\")\n","\n","df"],"metadata":{"id":"AvJFwNSW3ZHL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Flag the incorrect values as 0 and correct values as 1"],"metadata":{"id":"PUON9SEnGv3g"}},{"cell_type":"code","source":["df['flag'] = df['percent_correct'].map(lambda x: 1 if x > 0.5 else 0)\n","\n","df"],"metadata":{"id":"VRR75W3bzwpB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Distribution of correct and incorrect answers"],"metadata":{"id":"XhEOY_RX21Yy"}},{"cell_type":"code","source":["df_pct = df[\"is_correct\"].value_counts().to_frame(\"Counts\").reset_index()\n","df_pct"],"metadata":{"id":"Ikc9UOp20tkJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1 = df['is_correct'].value_counts().to_frame(\"Counts\").plot(kind='pie', subplots=True, autopct='%1.1f%%')\n","plt.title('Percentage of students that have answered the questions correctly')\n","plt.show()"],"metadata":{"id":"NrwuGS-p1lvp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Word frequency in Incorrect dataset"],"metadata":{"id":"o4vBFZtJ_NQC"}},{"cell_type":"code","source":["# Supress warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"xLvvCnzp82pQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Filter the Incorrect dataset"],"metadata":{"id":"WWd0CF49OcGK"}},{"cell_type":"code","source":["# What words or phrases appear more frequently in questions that students tend to do poorly on?\n","\n","df2 = df.copy()\n","\n","# Create the incorrect dataset\n","df_incorrect = df2[df2['flag'] == 0]\n","df_incorrect"],"metadata":{"id":"dTsM0opDyNHn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data cleaning"],"metadata":{"id":"aCTufDRR1mz2"}},{"cell_type":"code","source":["# Function to clean and preprocess text\n","def clean_text(text):\n","\n","    # Remove special characters and numbers\n","    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n","\n","    # Convert text to lowercase\n","    text = text.lower()\n","\n","    # Remove punctuation\n","    text = ''.join([char for char in text if char not in string.punctuation])\n","\n","\n","    # Tokenize text\n","    tokens = nltk.word_tokenize(text)\n","\n","    # Remove stopwords\n","    stop_words = set(stopwords.words('english'))\n","\n","    tokens = [word for word in tokens if word not in stop_words and not word.isdigit()]\n","\n","    #Stemming (you can replace with lemmatization if preferred)\n","    #stemmer = PorterStemmer()\n","\n","    #tokens = [stemmer.stem(word) for word in tokens]\n","\n","    # Create a lemmatizer object.\n","    lemmatizer = WordNetLemmatizer()\n","\n","   #Lemmatization\n","    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n","\n","    # Reconstruct cleaned text\n","    cleaned_text = ' '.join(tokens)\n","\n","    return cleaned_text\n"],"metadata":{"id":"nKTNi-JHvfJ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply the clean_text function to each review in the DataFrame\n","df2 = df1.copy()\n","\n","df_incorrect['Clean_text'] = df_incorrect['text'].apply(clean_text)\n","\n","# Print the cleaned reviews\n","df_incorrect"],"metadata":{"id":"6pydpTOBcjFd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Approach 1: Visualize the Word Cloud"],"metadata":{"id":"pKYOhIcC_r_h"}},{"cell_type":"code","source":["# Generate a word cloud for df_incorrect[\"Clean_text\"]\n","\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","\n","# Download the nltk stopwords if you haven't done so\n","nltk.download('stopwords')\n","\n","# Create a set of stopwords\n","stop_words = set(stopwords.words('english'))\n","\n","# Split the text into words and remove duplicates\n","word_list = str(df_incorrect[\"Tokenized_Text\"]).split()\n","unique_words =set(word_list)\n","\n","# Create a new text string with unique words\n","unique_text = \" \".join(unique_words)\n","\n","# Generate a word cloud with stop words\n","wordcloud = WordCloud(width=800, height=400, stopwords=stop_words, background_color='white').generate(unique_text)\n","\n","# plot the graph\n","plt.figure(figsize=(10,10))\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis('off')\n","plt.show()"],"metadata":{"id":"qUXN0tlhKGef"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Approach 2: Visualize Word frequency distribution of the Unigrams, Bigrams and Trigrams using NLTK"],"metadata":{"id":"42gkyrvWB7bn"}},{"cell_type":"markdown","source":["## Unigrams Word Frequency Distribution"],"metadata":{"id":"j42f0EjjrxBL"}},{"cell_type":"code","source":["import collections\n","from collections import Counter\n","from itertools import chain\n","\n","word_tokenize = nltk.word_tokenize\n","\n","# Tokenize the text column\n","df_incorrect['Tokenized_Text'] = df_incorrect['Clean_text'].apply(word_tokenize)\n","\n","# Print the tokenized text\n","corpus = df_incorrect['Tokenized_Text']\n","corpus = corpus.tolist()\n","# Flatten list of lists to a single list\n","tokens = list(chain(*corpus))\n","unique_freq = collections.Counter(tokens)\n","# Count each unique element\n","unique_freq_df = pd.DataFrame.from_dict(unique_freq, orient='index').reset_index() # Convert to dataframe\n","# Rename columns\n","unique_freq_df = unique_freq_df.rename(columns={'index': 'Token', 0: 'Count'})\n","# Sort by count\n","unique_freq_df.sort_values('Count', ascending=False, inplace=True)\n","unique_freq_df = unique_freq_df.head(20)\n","\n","unique_freq_df1 = unique_freq_df.reset_index(drop=True)\n","unique_freq_df2 = unique_freq_df1.set_index(\"Token\")\n","unique_freq_df2"],"metadata":{"id":"nwuNhJ0q8OrG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Plot the Top 20 Unigrams"],"metadata":{"id":"msZARicR-kiy"}},{"cell_type":"code","source":["#plt.colormaps()\n","unique_freq_df2.plot(kind=\"bar\", figsize= (18,5), grid=False, color = \"pink\")\n","plt.title(\"Word Frequency of Top 20 Unigrams in Incorrect Data\", size = 15)\n","plt.xlabel(\"Words\")\n","plt.ylabel(\"Frequency\")\n","plt.show()"],"metadata":{"id":"s02Cg0zoFb2J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Visualizing Bigrams Frequency Distribution"],"metadata":{"id":"NQySor_v_9h2"}},{"cell_type":"code","source":["# Generate bigrams from df_incorrect[\"Clean_text\"]\n","\n","bigram_list = [list(nltk.bigrams(text.split())) for text in df_incorrect['Clean_text']]\n","\n","# Create a Counter object to count the frequency of each bigram\n","bigram_count = collections.Counter(list(chain(*bigram_list)))\n","\n","# Convert the Counter object to a DataFrame\n","bigram_df = pd.DataFrame.from_dict(bigram_count, orient='index').reset_index()\n","\n","# Rename the columns\n","bigram_df = bigram_df.rename(columns={'index': 'Bigram', 0: 'Count'})\n","\n","# Sort the DataFrame by frequency in descending order\n","bigram_df.sort_values('Count', ascending=False, inplace=True)\n","\n","# Print the top 20 bigrams\n","bigram_df.head(20)"],"metadata":{"id":"S2-65QuHX_8S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bigram20 = bigram_df.head(20)\n","\n","# Plot the bigram_df\n","bigram20.plot(x=\"Bigram\", y=\"Count\", kind=\"bar\", figsize= (18,5), grid=False, color = \"pink\")\n","plt.title(\"Bigram Frequency of Top 20 Bigrams in Incorrect Data\")\n","plt.xlabel(\"Bigrams\")\n","plt.ylabel(\"Frequency\")\n","plt.show()"],"metadata":{"id":"h43IaSxPaCbH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Visualizing trigram frequency distribution"],"metadata":{"id":"yQADJboEbOJO"}},{"cell_type":"code","source":["\n","trigram_list = [list(nltk.trigrams(text.split())) for text in df_incorrect['Clean_text']]\n","\n","# Create a Counter object to count the frequency of each bigram\n","trigram_count = collections.Counter(list(chain(*trigram_list)))\n","\n","# Convert the Counter object to a DataFrame\n","trigram_df = pd.DataFrame.from_dict(trigram_count, orient='index').reset_index()\n","\n","# Rename the columns\n","trigram_df = trigram_df.rename(columns={'index': 'Trigram', 0: 'Count'})\n","\n","# Sort the DataFrame by frequency in descending order\n","trigram_df.sort_values('Count', ascending=False, inplace=True)\n","\n","# Print the top 20 bigrams\n","trigram_df.head(20)"],"metadata":{"id":"Zd4VMwJsbOVC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trigram20 = trigram_df.head(20)\n","\n","# Plot the bigram_df\n","trigram20.plot(x=\"Trigram\", y=\"Count\", kind=\"bar\", figsize= (18,5), grid=False, color = \"pink\")\n","plt.title(\"Trigram Frequency of Top 20 Trigrams in Incorrect Data\", size = 15)\n","plt.xlabel(\"Trigrams\")\n","plt.ylabel(\"Frequency\")\n","plt.show()"],"metadata":{"id":"S1PyBfCRbu1K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Approach 3: BERT Topic Modelling"],"metadata":{"id":"13DuqmODQu4y"}},{"cell_type":"code","source":["# Install bertopic\n","!pip install bertopic"],"metadata":{"id":"5woj-R_x1qCO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Try to import BERTopic\n","from bertopic import BERTopic"],"metadata":{"id":"vB13bJ913pzJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Data processing\n","import pandas as pd\n","import numpy as np\n","\n","# Text preprocessiong\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('omw-1.4')\n","nltk.download('wordnet')\n","wn = nltk.WordNetLemmatizer()\n","\n","# Topic model\n","from bertopic import BERTopic\n","\n","# Dimension reduction\n","from umap import UMAP"],"metadata":{"id":"HtfGVi5Y36zg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initiate UMAP\n","umap_model = UMAP(n_neighbors=15,\n","                  n_components=5,\n","                  min_dist=0.0,\n","                  metric='cosine',\n","                  random_state=100)\n","\n","# Initiate BERTopic\n","topic_model = BERTopic(umap_model=umap_model, language=\"english\", calculate_probabilities=True)\n","\n","# Run BERTopic model\n","topics, probabilities = topic_model.fit_transform(df_incorrect['Clean_text'])"],"metadata":{"id":"_iLmkkNp39hY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the list of topics\n","topic_model.get_topic_info()"],"metadata":{"id":"F3yWtR-C4w0x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get top 10 terms for a topic\n","topic_model.get_topic(1)"],"metadata":{"id":"H6Wt61414369"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize top topic keywords\n","topic_model.visualize_barchart(top_n_topics=8)"],"metadata":{"id":"N4sx3A9n5gSP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize intertopic distance\n","topic_model.visualize_topics()"],"metadata":{"id":"8U1d-ikE44hy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize similarity using heatmap\n","topic_model.visualize_heatmap()"],"metadata":{"id":"-NXeFpLw6U1p"},"execution_count":null,"outputs":[]}]}